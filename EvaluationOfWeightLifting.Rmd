---
title: "Evaluation of Weight Lifting Techniques"
author: "Philip Bulsink"
date: '2017-02-26'
output: html_document
---

```{r setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(ggplot2)
library(parallel)
library(doParallel)
library(randomForest)
library(gbm)
library(survival)
library(splines)
library(plyr)
library(ipred)
library(e1071)
library(MASS)
library(klaR)
set.seed(1)
```

#Synopsis 


#Introduction
Many people collect data on a wide range of aspects of their lives. This is particularly popular amongst individuals interested in personal health and fitness. A range of devices can track steps, sleep quality, heartrate, and other health parameters. However, the data collection is typically for quantification purposes, for example: comparing number of steps between days. However, with advanced machine learning techniques, and properly collected data, these movement detecting devices should be able to predict whether an activity such as weightlifting is being performed correctly. 

#Data Preparation
The data for this report comes from a study performed by Velloso *et. al.* which described in detail on [their site](http://groupware.les.inf.puc-rio.br/har). In this work, sensors were placed on a subject's upper arm, hand, belt, and on a dumbbell, while the subject performed weight lifting in the proper and in 4 common yet improper ways. A total of 6 subjects were used for this study. 
```{r loadData}
#Load training and testing data
lifting<-read.csv('./data/pml-training.csv', stringsAsFactors = FALSE)
```

In total, there are `r ncol(lifting)` columns of data, of which most but not all are predictors. By reading [the paper published with this data](http://groupware.les.inf.puc-rio.br/public/papers/2013.Velloso.QAR-WLE.pdf), we know that there are additional non-predictive variables (including subject information, data collection time, etc.) and the `classe` variable dictating the weightlifting type (correct (`classe` = 'A', or common error (`classe` = ['B','C','D',or 'E'])). 

Trimming the data to only contain the predictors we are interested will aid in our model development. We are interested in all variables related to 'belt', 'arm', 'dumbbell', and 'forearm'. We will split our data into a training and test set, with 70% of data for model training. We will also sub-split our training data into a training set and a validation set, to ensure that we can do proper predictions. Our validation set will be constructed of a 30% sample from our training set.

```{r dataPrep}
#Classe should be a factor variable
lifting$classe<-as.factor(lifting$classe)

#Extract only movement data & 'classe'
movementData<-grep('arm|dumbbell|belt|forearm|classe', names(lifting))
lifting<-lifting[,movementData]

#Split data into training and validation sets
inTest<-createDataPartition(y=lifting$classe, p=0.3, list=FALSE)
testing<-lifting[inTest,]
tv<-lifting[-inTest,]
inTrain<-createDataPartition(y=tv$classe, p=0.7, list=FALSE)
training<-tv[inTrain,]
validation<-tv[-inTrain,]
```

Many of the variables in the data set contain empty or NA values. We'll convert all data to numeric, and drop all the data types that are mostly NA values.
```{r dataCleanNA, warning=FALSE}
makeDataNumeric<-function(x){
    if (class(x) == 'character')
        x<-as.numeric(x)
    return(x)
}

testing[,-ncol(testing)]<-apply(testing[,-ncol(testing)], 2, makeDataNumeric)
training[,-ncol(training)]<-apply(training[,-ncol(training)], 2, makeDataNumeric)
validation[,-ncol(validation)]<-apply(validation[,-ncol(validation)], 2, makeDataNumeric)

naNames<-apply(training, 2, function(x) sum(is.na(x))/length(x) > 0.5)

naNames<-naNames[naNames]

testing<-testing[,!(colnames(testing) %in% names(naNames))]
training<-training[,!(colnames(training) %in% names(naNames))]
validation<-validation[,!(colnames(validation) %in% names(naNames))]
```

We've now cut our data set down to `r ncol(training)` columns of data, including the outcome `classe`. 

#Exploratory Analysis
This is a much more manageable size of data set to attempt to build models against. It's still very difficult to view any corellation between data points and the outcome `classe` that could indicate a reliable model. While `r ncol(training)` is many less variables than before, a correllation plot matrix is `r ncol(training)^2` images, and would be impossible to look at with any detail.

One option in such circumstances it to look at a 'correllation plot'. R has a tool for this in the `corrplot` package.
```{r corrplot}
library(corrplot)
corrplot(cor(training[,-ncol(training)], use='complete.obs'), type = 'lower', order = 'hclust')
```

From this we can see good correllation (darker blue) between similar points (eg. all of the forearm points are correllated), but poor correllation appears as white or red elsewhere. It may be difficult to classify this data intuitively, so a random forest or other boosted or bagged modesl may be needed. 

#Model Building
We'll start by building a random forest data model. We'll use parallel processing to speed this up, as it can be a slow step.

```{r first_rf_model_build, cache=TRUE}
cluster <- makeCluster(detectCores() - 1)
registerDoParallel(cluster)

fitControl <- trainControl(method = "cv",
                           number = 10,
                           allowParallel = TRUE)
set.seed(1)
model1 <- train(classe~., method="rf", data=training, trControl=fitControl)

stopCluster(cluster)
registerDoSEQ()
```

We can see how accurate this first model is by comparing against the validation set:

```{r rf_model_confusion}
cm1<-confusionMatrix(predict(model1, validation), validation$classe)
cm1
```

More models can be compared as well: `model2` will be built after preprocessing the data with principle components analysis, `model3` is a boosted tree method, `model4` is a bagged tree model, `model5` is a linear discriminate analysis, and `model6` uses naive bayes.  
```{r extra_model_build, cache=TRUE}
cluster <- makeCluster(detectCores() - 1)
registerDoParallel(cluster)

fitControl <- trainControl(method = "cv", number = 10, allowParallel = TRUE)
fitControl2 <- trainControl(allowParallel = TRUE)

set.seed(1)
model2 <- train(classe~., method="rf", data=training, trControl=fitControl, preprocess="pca")
model3 <- train(classe~., method="gbm", data=training, trControl=fitControl2, verbose=FALSE)
model4 <- train(classe~., method="treebag", data=training, trControl=fitControl2)
model5 <- train(classe~., method="lda", data=training, trControl=fitControl2)
model6 <- train(classe~., method="nb", data=training, trControl=fitControl2)

stopCluster(cluster)
registerDoSEQ()
```

```{r extra_model_confusion}
cm2<-confusionMatrix(predict(model2, validation), validation$classe)
cm3<-confusionMatrix(predict(model3, validation), validation$classe)
cm4<-confusionMatrix(predict(model4, validation), validation$classe)
cm5<-confusionMatrix(predict(model5, validation), validation$classe)
cm6<-confusionMatrix(predict(model6, validation), validation$classe)
```

We can see that the accuracy of the first model was `r cm1$overall[[1]]`, the preprocessed with pca version had accuracy of `r cm2$overall[[1]]`, the boosted tree method was `r cm3$overall[[1]]`, the bagged method `r cm4$overall[[1]]`, the lda model `r cm5$overall[[1]]`, and the naive bayes `r cm6$overall[[1]]`. 

We can combine each model together to create a more accurate predictor than each model alone would produce. This then feeds into a generalized additive model to predict a combined outcome. 
```{r combine}
pred1<-predict(model1, training)
pred2<-predict(model2, training)
pred3<-predict(model3, training)
pred4<-predict(model4, training)
pred5<-predict(model5, training)
pred6<-predict(model6, training)
predOutcome<-training$classe
trainDF<-data.frame(pred1,pred2,pred3,pred4,pred5,pred6,classe=predOutcome)
trainDF2<-data.frame(pred1,pred2,pred3,pred4, classe=predOutcome)
combModFit<-train(classe~., data=trainDF, method='gam')
combModFit2<-train(classe~., data=trainDF2, method='gam')
```

We can compare this to the validation data frame, prepared similarly as the training data frame above.
```{r combine_val, ech0=FALSE}
val1<-predict(model1, validation)
val2<-predict(model2, validation)
val3<-predict(model3, validation)
val4<-predict(model4, validation)
val5<-predict(model5, validation)
val6<-predict(model6, validation)
valOutcome<-validation$classe
valDF<-data.frame(pred1=val1,pred2=val2,pred3=val3,pred4=val4,pred5=val5,pred6=val6,classe=valOutcome)
```

The accuracy of this combination of methods is:
```{r combine_matrix}
cfComb<-confusionMatrix(predict(combModFit, valDF), valDF$classe)
cfComb2<-confusionMatrix(predict(combModFit2, valDF), valDF$classe)
cfComb$overall[1]
cfComb$overall[2]
```

#Model Evaluation
Thus, the combined methods perform better than each of the individuals on the validation set, but how do they do on the test set?
```{r test}
test1<-predict(model1, testing)
test2<-predict(model2, testing)
test3<-predict(model3, testing)
test4<-predict(model4, testing)
test5<-predict(model5, testing)
test6<-predict(model6, testing)
testOutcome<-testing$classe
testDF<-data.frame(pred1=test1,pred2=test2,pred3=test3,pred4=test4,pred5=test5,pred6=test6,classe=testOutcome)
cfTest<-confusionMatrix(predict(combModFit, testDF), testDF$classe)
cfTest
```

#Conclusions

#Appendix 1: Quiz Results
Additional testing data is provided for the purposes of a quiz. This data will be loaded, cleaned, and predicted upon here:

```{r quiz}
quizing<-read.csv('./data/pml-testing.csv', stringsAsFactors = FALSE)
problem_id<-quizing$problem_id

#Extract only movement data
quizing<-quizing[,movementData]

#Clean Data
quizing[,-ncol(quizing)]<-apply(quizing[,-ncol(quizing)], 2, makeDataNumeric)

quizing<-quizing[,!(colnames(quizing) %in% names(naNames))]

quiz1<-predict(model1, testing)
quiz2<-predict(model2, testing)
quiz3<-predict(model3, testing)
quiz4<-predict(model4, testing)
quiz5<-predict(model5, testing)
quiz6<-predict(model6, testing)
quizDF<-data.frame(pred1=test1,pred2=test2,pred3=test3,pred4=test4,pred5=test5,pred6=test6)

quizpredict<-predict(combModFit, quizDF)

as.data.frame(problem_id, quizpredict)
```

#Appendix 2: R Session Data
```{r session}
sessionInfo()
```
