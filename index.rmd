---
title: "Evaluation of Weight Lifting Techniques"
author: "Philip Bulsink"
date: '2017-02-26'
output: html_document
---

```{r setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE)
options(tidy=TRUE, scipen = 0, digits = 4)
library(lattice)
library(ggplot2)
library(ggfortify)
library(cowplot)
library(caret)
library(parallel)
library(foreach)
library(doParallel)
library(randomForest)
library(survival)
library(gbm)
library(splines)
library(plyr)
library(ipred)
library(e1071)
library(MASS)
library(reshape2)
set.seed(1)
```

#Synopsis 
Weight training is a popular activity, but requires proper training to do correctly. With data collected from a few motion sensors, we can analyze the form of the weight lifting and accurately identify if it is being performed correctly or incorrectly. 

#Introduction
Many people collect data on a wide range of aspects of their lives. This is particularly popular amongst individuals interested in personal health and fitness. A range of devices can track steps, sleep quality, heartrate, and other health parameters. However, the data collection is typically for quantification purposes, for example: comparing number of steps between days. However, with advanced machine learning techniques, and properly collected data, these movement detecting devices should be able to predict whether an activity such as weightlifting is being performed correctly. 

#Data Preparation
The data for this report comes from a study performed by Velloso *et. al.* which described in detail on [their site](http://groupware.les.inf.puc-rio.br/har). In this work, sensors were placed on a subject's upper arm, hand, belt, and on a dumbbell, while the subject performed weight lifting in the proper and in 4 common yet improper ways. A total of 6 subjects were used for this study. 
```{r loadData}
#Load training and testing data
lifting<-read.csv('./data/pml-training.csv', stringsAsFactors = FALSE)
```

In total, there are `r ncol(lifting)` columns of data, of which most but not all are predictors. By reading [the paper published with this data](http://groupware.les.inf.puc-rio.br/public/papers/2013.Velloso.QAR-WLE.pdf), we know that there are additional non-predictive variables (including subject information, data collection time, etc.) and the `classe` variable dictating the weightlifting type (correct (`classe` = 'A', or common error (`classe` = ['B','C','D',or 'E'])). 

Trimming the data to only contain the predictors we are interested will aid in our model development. We are interested in all variables related to 'belt', 'arm', 'dumbbell', and 'forearm'. We will split our data into a training and test set, with 70% of data for model training. We will also sub-split our training data into a training set and a validation set, to ensure that we can do proper predictions. Our validation set will be constructed of a 30% sample from our training set.

```{r dataPrep}
#Classe should be a factor variable
lifting$classe<-as.factor(lifting$classe)

#Extract only movement data & 'classe'
movementData<-grep('arm|dumbbell|belt|forearm|classe', names(lifting))
lifting<-lifting[,movementData]

#Ensure data is numeric (don't affect classe)
lifting[,-ncol(lifting)]<-apply(lifting[,-ncol(lifting)], 2, function(x) as.numeric(x))

#Split data into training and validation sets
inTest<-createDataPartition(y=lifting$classe, p=0.3, list=FALSE)
testing<-lifting[inTest,]
tv<-lifting[-inTest,]
inTrain<-createDataPartition(y=tv$classe, p=0.7, list=FALSE)
training<-tv[inTrain,]
validation<-tv[-inTrain,]
```

Many of the variables in the data set contain empty or NA values. We'll drop all the data types that are mostly NA values.
```{r dataCleanNA, warning=FALSE}
naNames<-apply(training, 2, function(x) sum(is.na(x))/length(x) > 0.5)

naNames<-naNames[naNames]

testing<-testing[,!(colnames(testing) %in% names(naNames))]
training<-training[,!(colnames(training) %in% names(naNames))]
validation<-validation[,!(colnames(validation) %in% names(naNames))]
```

We've now cut our data set down to `r ncol(training)` columns of data, including the outcome `classe`. 

#Exploratory Analysis
This is a much more manageable size of data set to attempt to build models against. It's still very difficult to view any corellation between data points and the outcome `classe` that could indicate a reliable model. While `r ncol(training)` is many less variables than before, a correllation plot matrix is `r ncol(training)^2` images, and would be impossible to look at with any detail.

One option in such circumstances it to look at a 'correllation plot'. This is built using `ggplot2` and `reshape2` packages, melting the correllation matrix to a long data frame before building the plot.
```{r corrplot}
ggplot(data=melt(cor(training[,-ncol(training)])), aes(x=Var1, y=Var2, fill=value)) + 
    geom_tile() + 
    scale_fill_distiller(palette = "RdBu", name = "Cor") + 
    ggtitle('Correllation Plot') + 
    xlab('') + 
    ylab('') + 
    theme_bw() + 
    theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))
```

From this we can see good correllation (darker red or blue) between similar points (eg. all of the forearm points are correllated), but poor correllation appears as lighter red or blue elsewhere. It may be difficult to classify this data intuitively, so a random forest or other boosted or bagged model may be needed. 

It may be possible to attempt to understand the data by performing principal component analysis (PCA) on the data. This may also improve the performance of models when we develop them. We can take a look at the first few PCA factors to see if they provide any grouping of variable by activity classs.

This is built using `ggfortify` to produce plots, and `cowplot` to draw the plots together on one panel. 
```{r pca_plots, message=FALSE}
library(ggfortify)
pca_a<-autoplot(prcomp(training[,-ncol(training)], scale=TRUE), data=training, colour='classe', x=1, y=2, size=0.1, alpha=0.5) + 
    theme(legend.position="bottom", legend.title = element_text(colour="white"), legend.text = element_text(colour="white"))
pca_b<-autoplot(prcomp(training[,-ncol(training)], scale=TRUE), data=training, colour='classe', x=1, y=3, size=0.1, alpha=0.5) + 
    theme(legend.position="bottom", legend.title = element_text(colour="white"), legend.text = element_text(colour="white"))
pca_c<-autoplot(prcomp(training[,-ncol(training)], scale=TRUE), data=training, colour='classe', x=2, y=3, size=0.1, alpha=0.5) + 
    theme(legend.position="bottom")

library(cowplot)
p<-plot_grid(pca_a, pca_b, pca_c, nrow = 1, align='h')
title <- ggdraw() + draw_label("PCA Plots of Components 1, 2, and 3", fontface='bold')
plot_grid(title, p, ncol=1, rel_heights=c(0.1, 1))
```

We can see that, while the PCA analysis can split the data into discrete groups, this splitting does not show clustering of specific classe factors to any one area. A k-means clustering technique will likely not be a good choice for a model. 

#Model Building
We'll start by building a random forest data model. We'll use parallel processing to speed this up, as it can be a slow step.

```{r par start}
cluster <- makeCluster(detectCores() - 1)
registerDoParallel(cluster)

fitControl <- trainControl(method = "cv",
                           number = 20,
                           allowParallel = TRUE)
```
```{r first_rf_model_build, cache=TRUE}
set.seed(1)
model1 <- train(classe~., method="rf", data=training, trControl=fitControl)
```

We can see how accurate this first model is by comparing against the validation set:

```{r rf_model_confusion}
cm1<-confusionMatrix(predict(model1, validation), validation$classe)
cm1
```

More models can be compared as well: `model2` will be built after preprocessing the data with principle components analysis as above, `model3` is a boosted tree method, `model4` is a bagged tree model, `model5` is a linear discriminate analysis, and `model6` uses a recursive partitioning tree model.
```{r extra_model_build, cache=TRUE}
set.seed(1)
model2 <- train(classe~., method="rf", data=training, trControl=fitControl, preprocess="pca")
model3 <- train(classe~., method="gbm", data=training, trControl=fitControl, verbose=FALSE)
model4 <- train(classe~., method="treebag", data=training, trControl=fitControl)
model5 <- train(classe~., method="lda", data=training, trControl=fitControl)
model6 <- train(classe~., method="rpart", data=training, trControl=fitControl)
```

```{r extra_model_confusion}
cm2<-confusionMatrix(predict(model2, validation), validation$classe)
cm3<-confusionMatrix(predict(model3, validation), validation$classe)
cm4<-confusionMatrix(predict(model4, validation), validation$classe)
cm5<-confusionMatrix(predict(model5, validation), validation$classe)
cm6<-confusionMatrix(predict(model6, validation), validation$classe)
```

We can see that the accuracy of the first model was `r cm1$overall[[1]]`, the preprocessed with pca version had accuracy of `r cm2$overall[[1]]`, the boosted tree method was `r cm3$overall[[1]]`, the bagged method `r cm4$overall[[1]]`, the lda model `r cm5$overall[[1]]`, and the rpart `r cm6$overall[[1]]`. With the poor performance of the last two models, we know that lda and rpart are less powerful for this type of data.

We can combine a subset of the models together to create a more accurate predictor than each model alone would produce. As well, although we performed preprocessing PCA on the data for model2, the model is nearly equivalent to model 1. 

This then feeds into a gradient boosting model to predict a combined outcome. 
```{r combine, cache=TRUE, message=FALSE, warning=FALSE}
pred1<-predict(model1, training)
pred2<-predict(model2, training)
pred3<-predict(model3, training)
pred4<-predict(model4, training)
predOutcome<-training$classe
trainDF<-data.frame(pred1,pred2,pred3,pred4,classe=predOutcome)
combModFit<-train(classe~., data=trainDF, method='gbm', verbose=FALSE, trControl=fitControl)
```
```{r par_end}
stopCluster(cluster)
registerDoSEQ()
```

We can compare this to the validation data frame, prepared similarly as the training data frame above.
```{r combine_val, echo=FALSE}
val1<-predict(model1, validation)
val2<-predict(model2, validation)
val3<-predict(model3, validation)
val4<-predict(model4, validation)
valOutcome<-validation$classe
valDF<-data.frame(pred1=val1,pred2=val2,pred3=val3,pred4=val4,classe=valOutcome)
```

The accuracy of this combination of methods is:
```{r combine_matrix}
cfComb<-confusionMatrix(predict(combModFit, valDF), valDF$classe)
cfComb$overall[1]
```

#Model Evaluation
Thus, the combined methods perform as good or better than each of the individuals on the validation set, but how do they do on the test set? 
```{r test}
test1<-predict(model1, testing)
test2<-predict(model2, testing)
test3<-predict(model3, testing)
test4<-predict(model4, testing)
testOutcome<-testing$classe
testDF<-data.frame(pred1=test1,pred2=test2,pred3=test3,pred4=test4,classe=testOutcome)
cmTest<-confusionMatrix(predict(combModFit, testDF), testDF$classe)
cmTest
```

So, we get an overall test set accuracy of `r cmTest$overall[[1]]`, with 95% confidence range from `r cmTest$overall[[3]]` to `r cmTest$overall[[4]]`. Note that, with a limited number of testing cases available, improving the prediction's accuracy above where it misclassifies only a handful of the `r nrow(testing)` test samples is incredibly difficult. 

#Conclusions
In conclusion, we can, with `r cmTest$overall[[1]]` accuracy, detect which way an excercise is being performed. With this knowledge, an amateur weight trainer would be able to not only quantify, but qualify the workout that they are performing, to ensure that their form is correct. This can reduce the chance of injury and maximize the impact of the workout. 

#Appendix 1: Quiz Results
Additional testing data is provided for the purposes of a quiz. This data will be loaded, cleaned, and predicted upon here:

```{r quiz}
quizing<-read.csv('./data/pml-testing.csv', stringsAsFactors = FALSE)
problem_id<-quizing$problem_id

#Extract only movement data
quizing<-quizing[,movementData]

#Clean Data
quizing[,-ncol(quizing)]<-apply(quizing[,-ncol(quizing)], 2, function(x) as.numeric(x))

quizing<-quizing[,!(colnames(quizing) %in% names(naNames))]

quiz1<-predict(model1, quizing)
quiz2<-predict(model2, quizing)
quiz3<-predict(model3, quizing)
quiz4<-predict(model4, quizing)
quizDF<-data.frame(pred1=quiz1,pred2=quiz2,pred3=quiz3, pred4=quiz4)

quizpredict<-predict(combModFit, quizDF)

data.frame(problem_id, quizpredict)
```

#Appendix 2: R Session Data
```{r session}
sessionInfo()
```
